{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ttn9171/Portfolio/blob/main/Modeling/UC2%3A%20NBA/Feature%20Extractions%20and%20Engineer/GA_Sessions_and_UTM_Extractions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Connection"
      ],
      "metadata": {
        "id": "ThOvI4nx8cEy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DlPp07Y3Kmp4",
        "outputId": "bccc9a6a-cd86-4a57-d026-cb39379c0168"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "\r            \rGet:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "\r0% [Connecting to archive.ubuntu.com (185.125.190.81)] [Connecting to security.ubuntu.com (185.125.1\r0% [Connecting to archive.ubuntu.com (185.125.190.81)] [Connecting to security.ubuntu.com (185.125.1\r                                                                                                    \rHit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\r0% [Waiting for headers] [Waiting for headers] [Connected to ppa.launchpadcontent.net (185.125.190.8\r                                                                                                    \rGet:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:7 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [69.9 kB]\n",
            "Get:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,378 kB]\n",
            "Get:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
            "Get:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:13 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,748 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,692 kB]\n",
            "Get:15 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,673 kB]\n",
            "Get:16 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [33.6 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,000 kB]\n",
            "Get:18 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy/main amd64 Packages [46.8 kB]\n",
            "Get:19 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,236 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,534 kB]\n",
            "Fetched 21.8 MB in 2s (9,013 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "curl is already the newest version (7.81.0-1ubuntu1.20).\n",
            "gnupg is already the newest version (2.2.27-3ubuntu2.1).\n",
            "gnupg set to manually installed.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 32 not upgraded.\n",
            "deb [signed-by=/usr/share/keyrings/cloud.google.gpg] https://packages.cloud.google.com/apt gcsfuse-jammy main\n",
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Get:4 https://packages.cloud.google.com/apt gcsfuse-jammy InRelease [1,227 B]\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:6 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:12 https://packages.cloud.google.com/apt gcsfuse-jammy/main amd64 Packages [34.6 kB]\n",
            "Get:13 https://packages.cloud.google.com/apt gcsfuse-jammy/main all Packages [750 B]\n",
            "Fetched 36.6 kB in 1s (27.9 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "The following NEW packages will be installed:\n",
            "  gcsfuse\n",
            "0 upgraded, 1 newly installed, 0 to remove and 32 not upgraded.\n",
            "Need to get 15.1 MB of archives.\n",
            "After this operation, 0 B of additional disk space will be used.\n",
            "Get:1 https://packages.cloud.google.com/apt gcsfuse-jammy/main amd64 gcsfuse amd64 2.11.0 [15.1 MB]\n",
            "Fetched 15.1 MB in 0s (35.1 MB/s)\n",
            "Selecting previously unselected package gcsfuse.\n",
            "(Reading database ... 125044 files and directories currently installed.)\n",
            "Preparing to unpack .../gcsfuse_2.11.0_amd64.deb ...\n",
            "Unpacking gcsfuse (2.11.0) ...\n",
            "Setting up gcsfuse (2.11.0) ...\n",
            "{\"timestamp\":{\"seconds\":1742239080,\"nanos\":390006154},\"severity\":\"INFO\",\"message\":\"Start gcsfuse/2.11.0 (Go version go1.24.0) for app \\\"\\\" using mount point: /content/gcs\\n\"}\n",
            "{\"timestamp\":{\"seconds\":1742239080,\"nanos\":390472549},\"severity\":\"INFO\",\"message\":\"GCSFuse config\",\"config\":{\"AppName\":\"\",\"CacheDir\":\"\",\"Debug\":{\"ExitOnInvariantViolation\":false,\"Fuse\":false,\"Gcs\":false,\"LogMutex\":false},\"EnableAtomicRenameObject\":false,\"EnableHns\":true,\"FileCache\":{\"CacheFileForRangeRead\":false,\"DownloadChunkSizeMb\":50,\"EnableCrc\":false,\"EnableODirect\":false,\"EnableParallelDownloads\":false,\"ExperimentalParallelDownloadsDefaultOn\":false,\"MaxParallelDownloads\":16,\"MaxSizeMb\":-1,\"ParallelDownloadsPerFile\":16,\"WriteBufferSize\":4194304},\"FileSystem\":{\"DirMode\":\"755\",\"DisableParallelDirops\":false,\"FileMode\":\"644\",\"FuseOptions\":[],\"Gid\":-1,\"HandleSigterm\":true,\"IgnoreInterrupts\":true,\"KernelListCacheTtlSecs\":0,\"PreconditionErrors\":true,\"RenameDirLimit\":0,\"TempDir\":\"\",\"Uid\":-1},\"Foreground\":false,\"GcsAuth\":{\"AnonymousAccess\":false,\"KeyFile\":\"\",\"ReuseTokenFromUrl\":true,\"TokenUrl\":\"\"},\"GcsConnection\":{\"BillingProject\":\"\",\"ClientProtocol\":\"http1\",\"CustomEndpoint\":\"\",\"ExperimentalEnableJsonRead\":false,\"GrpcConnPoolSize\":1,\"HttpClientTimeout\":0,\"LimitBytesPerSec\":-1,\"LimitOpsPerSec\":-1,\"MaxConnsPerHost\":0,\"MaxIdleConnsPerHost\":100,\"SequentialReadSizeMb\":200},\"GcsRetries\":{\"ChunkTransferTimeoutSecs\":10,\"MaxRetryAttempts\":0,\"MaxRetrySleep\":30000000000,\"Multiplier\":2,\"ReadStall\":{\"Enable\":false,\"InitialReqTimeout\":20000000000,\"MaxReqTimeout\":1200000000000,\"MinReqTimeout\":1500000000,\"ReqIncreaseRate\":15,\"ReqTargetPercentile\":0.99}},\"ImplicitDirs\":false,\"List\":{\"EnableEmptyManagedFolders\":false},\"Logging\":{\"FilePath\":\"\",\"Format\":\"json\",\"LogRotate\":{\"BackupFileCount\":10,\"Compress\":true,\"MaxFileSizeMb\":512},\"Severity\":\"INFO\"},\"MetadataCache\":{\"DeprecatedStatCacheCapacity\":20460,\"DeprecatedStatCacheTtl\":60000000000,\"DeprecatedTypeCacheTtl\":60000000000,\"EnableNonexistentTypeCache\":false,\"ExperimentalMetadataPrefetchOnMount\":\"disabled\",\"NegativeTtlSecs\":5,\"StatCacheMaxSizeMb\":32,\"TtlSecs\":60,\"TypeCacheMaxSizeMb\":4},\"Metrics\":{\"CloudMetricsExportIntervalSecs\":0,\"EnableOtel\":true,\"PrometheusPort\":0,\"StackdriverExportInterval\":0},\"Monitoring\":{\"ExperimentalOpentelemetryCollectorAddress\":\"\",\"ExperimentalTracingMode\":\"\",\"ExperimentalTracingSamplingRatio\":0},\"OnlyDir\":\"\",\"Write\":{\"BlockSizeMb\":33554432,\"CreateEmptyFile\":false,\"EnableStreamingWrites\":false,\"GlobalMaxBlocks\":9223372036854775807,\"MaxBlocksPerFile\":1}}}\n",
            "{\"timestamp\":{\"seconds\":1742239081,\"nanos\":53225153},\"severity\":\"INFO\",\"message\":\"File system has been successfully mounted.\"}\n"
          ]
        }
      ],
      "source": [
        "!apt-get update\n",
        "!apt-get install -y -q curl gnupg\n",
        "\n",
        "!curl -fsSL https://packages.cloud.google.com/apt/doc/apt-key.gpg | gpg --dearmor -o /usr/share/keyrings/cloud.google.gpg\n",
        "!echo \"deb [signed-by=/usr/share/keyrings/cloud.google.gpg] https://packages.cloud.google.com/apt gcsfuse-jammy main\" | tee /etc/apt/sources.list.d/gcsfuse.list\n",
        "!apt-get update\n",
        "\n",
        "!apt-get install -y -q gcsfuse\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "import os\n",
        "project_id = 'capstone-aldo'\n",
        "os.environ['Aldo_Capstone'] = project_id\n",
        "\n",
        "from google.cloud import storage\n",
        "\n",
        "client = storage.Client(project=project_id)\n",
        "bucket = 'mma-capstone'\n",
        "\n",
        "os.makedirs('/content/gcs', exist_ok=True)\n",
        "!gcsfuse {bucket} /content/gcs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extract Number of Web Visits and Interactions"
      ],
      "metadata": {
        "id": "zrZ_4VfkNAn5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import pyarrow.parquet as pq\n",
        "import gcsfs\n",
        "import gc\n",
        "import time\n",
        "import subprocess\n",
        "import os\n",
        "\n",
        "from typing import List\n",
        "\n",
        "def run_email_traffic_pipeline(year: int, country: str, bucket_name: str):\n",
        "    base_path = f\"gs://{bucket_name}/GA/ga_sample1/banner=ALDO_{country}/year={year}/final/year_{year}.parquet\"\n",
        "    gcs_output_file = f\"gs://{bucket_name}/data_preprocessed/visit_counts_{country}_{year}.csv\"\n",
        "    local_temp_file = f\"/tmp/temp_visit_counts_{year}.csv\"\n",
        "\n",
        "    columns_for_visits = [\"fullvisitor_id\", \"page_path\", \"unique_session_id\"]\n",
        "    chunksize = 500000\n",
        "\n",
        "    email_traffic_pattern = re.compile(\n",
        "        r'(utm_medium=email|sc_src=email[\\w\\d_-]*|cm_mmc=VIPClubEmails)',\n",
        "        re.IGNORECASE\n",
        "    )\n",
        "\n",
        "    def detect_email_traffic(url):\n",
        "        return bool(email_traffic_pattern.search(str(url))) if pd.notnull(url) else False\n",
        "\n",
        "    print(f\"\\n🚀 Starting {year} Data Processing...\")\n",
        "    fs = gcsfs.GCSFileSystem()\n",
        "\n",
        "    print(f\"\\n🔹 Processing year: {year} in batches...\")\n",
        "    parquet_file = pq.ParquetFile(fs.open(base_path, \"rb\"))\n",
        "\n",
        "    chunk_list = []\n",
        "    for chunk_number, batch in enumerate(parquet_file.iter_batches(batch_size=chunksize, columns=columns_for_visits), start=1):\n",
        "        start_time = time.time()\n",
        "        chunk = batch.to_pandas()\n",
        "        print(f\"\\n✅ Processing Chunk {chunk_number} - Rows: {chunk.shape[0]}\")\n",
        "\n",
        "        chunk[\"is_email_traffic\"] = chunk[\"page_path\"].apply(detect_email_traffic)\n",
        "        chunk_list.append(chunk)\n",
        "\n",
        "        print(f\"✅ Chunk {chunk_number} Processed in {time.time() - start_time:.2f} seconds\")\n",
        "        gc.collect()\n",
        "\n",
        "    if chunk_list:\n",
        "        df_2023 = pd.concat(chunk_list, ignore_index=True).dropna(subset=[\"fullvisitor_id\"])\n",
        "\n",
        "        df_sessions = df_2023.groupby([\"fullvisitor_id\", \"unique_session_id\"], as_index=False).agg({\n",
        "            \"is_email_traffic\": \"max\",\n",
        "            \"page_path\": \"count\"\n",
        "        }).rename(columns={\"page_path\": \"session_interactions\"})\n",
        "\n",
        "        df_grouped = df_sessions.groupby(\"fullvisitor_id\", as_index=False).agg({\n",
        "            \"unique_session_id\": \"nunique\",\n",
        "            \"is_email_traffic\": \"sum\",\n",
        "            \"session_interactions\": \"sum\"\n",
        "        }).rename(columns={\n",
        "            \"unique_session_id\": \"total_visits\",\n",
        "            \"is_email_traffic\": \"email_visits\",\n",
        "            \"session_interactions\": \"total_interactions\"\n",
        "        })\n",
        "\n",
        "        df_grouped[\"avg_interactions_per_session\"] = (\n",
        "            df_grouped[\"total_interactions\"] / df_grouped[\"total_visits\"].replace(0, 1)\n",
        "        )\n",
        "        df_grouped[\"country\"] = f\"ALDO_{country}\"\n",
        "\n",
        "        df_grouped = df_grouped.astype({\n",
        "            \"total_visits\": int,\n",
        "            \"email_visits\": int,\n",
        "            \"total_interactions\": int\n",
        "        })\n",
        "\n",
        "        df_grouped.to_csv(local_temp_file, index=False)\n",
        "\n",
        "        print(\"\\n📤 Uploading Final Processed Data to GCS...\")\n",
        "        upload_result = subprocess.run([\"gsutil\", \"cp\", local_temp_file, gcs_output_file], capture_output=True, text=True)\n",
        "\n",
        "        if upload_result.returncode == 0:\n",
        "            print(\"Final Data Successfully Uploaded to GCS!\")\n",
        "            os.remove(local_temp_file)\n",
        "        else:\n",
        "            print(\"Upload Failed! Debugging Output Below:\")\n",
        "            print(upload_result.stderr)\n",
        "\n",
        "        print(\"\\n🎉 Processing Completed Successfully! Data saved at:\", gcs_output_file)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_email_traffic_pipeline(year=2023, country=\"CA\", bucket_name=\"mma-capstone\")\n",
        "    run_email_traffic_pipeline(year=2023, country=\"US\", bucket_name=\"mma-capstone\")\n"
      ],
      "metadata": {
        "id": "A6S8sMgarhtr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extract Events per Session (view product, add to cart, etc.)\n",
        "\n"
      ],
      "metadata": {
        "id": "xJW6_Q56vcCd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import pyarrow.parquet as pq\n",
        "import gcsfs\n",
        "import gc\n",
        "import time\n",
        "import subprocess\n",
        "import os\n",
        "\n",
        "# GCS File Configuration\n",
        "YEAR = 2023\n",
        "COUNTRY = \"US\" #Replace region as needed (US or CA)\n",
        "BUCKET_NAME = \"mma-capstone\"\n",
        "BASE_PATH = f\"gs://{BUCKET_NAME}/GA/ga_sample1/banner=ALDO_{COUNTRY}/year={YEAR}/final/year_{YEAR}.parquet\"\n",
        "GCS_OUTPUT_FILE = f\"gs://{BUCKET_NAME}/data_preprocessed/session_event_{COUNTRY}_{YEAR}.csv\"\n",
        "LOCAL_TEMP_FILE = \"/tmp/session_event_analysis.csv\"\n",
        "\n",
        "# Define columns to load\n",
        "columns_to_load = [\"fullvisitor_id\", \"unique_session_id\", \"event_action\", \"date\"]\n",
        "\n",
        "# Categorical keywords for event classification\n",
        "checkout_keywords = {\n",
        "    \"checkout with Apple Pay\", \"checkout checkbox check\", \"fast-checkout\",\n",
        "    \"checkout step displayed\", \"checkout\", \"Expand Section Checkout\"\n",
        "}\n",
        "view_product_keywords = {\"view product detail\"}\n",
        "add_to_cart_keywords = {\"add to cart\"}\n",
        "purchase_keywords = {\"purchase\"}\n",
        "\n",
        "# ✅ Function to classify event actions\n",
        "def classify_event(event):\n",
        "    event = str(event).lower()  # Normalize text\n",
        "    return {\n",
        "        \"view_product_detail\": any(keyword in event for keyword in view_product_keywords),\n",
        "        \"add_to_cart\": any(keyword in event for keyword in add_to_cart_keywords),\n",
        "        \"checkout\": any(keyword in event for keyword in checkout_keywords),\n",
        "        \"purchase\": any(keyword in event for keyword in purchase_keywords)\n",
        "    }\n",
        "\n",
        "fs = gcsfs.GCSFileSystem()  # Initialize GCS filesystem\n",
        "parquet_file = pq.ParquetFile(fs.open(BASE_PATH, \"rb\"))\n",
        "\n",
        "# Load data in chunks\n",
        "chunksize = 500000\n",
        "chunk_number = 0\n",
        "processed_chunks = []  # Store processed chunks\n",
        "\n",
        "for batch in parquet_file.iter_batches(batch_size=chunksize, columns=columns_to_load):\n",
        "    start_time = time.time()\n",
        "    chunk_number += 1\n",
        "\n",
        "    df_chunk = batch.to_pandas()\n",
        "    print(f\"\\n Processing Chunk {chunk_number} - Rows: {df_chunk.shape[0]}\")\n",
        "\n",
        "    # ✅ Ensure `fullvisitor_id` and `unique_session_id` are treated as strings\n",
        "    df_chunk[\"fullvisitor_id\"] = df_chunk[\"fullvisitor_id\"].astype(str)\n",
        "    df_chunk[\"unique_session_id\"] = df_chunk[\"unique_session_id\"].astype(str)\n",
        "\n",
        "    # ✅ Apply event classification\n",
        "    classified_data = df_chunk[\"event_action\"].apply(classify_event).apply(pd.Series)\n",
        "    df_chunk = df_chunk.join(classified_data)\n",
        "\n",
        "    # ✅ Keep only necessary columns\n",
        "    df_chunk = df_chunk[[\"date\",\"fullvisitor_id\", \"unique_session_id\", \"view_product_detail\", \"add_to_cart\", \"checkout\", \"purchase\"]]\n",
        "\n",
        "    # ✅ Store processed chunk\n",
        "    processed_chunks.append(df_chunk)\n",
        "\n",
        "    # ✅ Free up memory\n",
        "    del df_chunk, classified_data, batch\n",
        "    gc.collect()\n",
        "\n",
        "    print(f\"Chunk {chunk_number} Processed in {time.time() - start_time:.2f} seconds\")\n",
        "\n",
        "# Concatenate all processed chunks into a single DataFrame\n",
        "df_final = pd.concat(processed_chunks, ignore_index=True)\n",
        "df_final = df_final.dropna(subset=[\"unique_session_id\"])\n",
        "\n",
        "\n",
        "del processed_chunks\n",
        "gc.collect()\n",
        "\n",
        "\n",
        "# ✅ Group AFTER processing all chunks (Avoids Double Counting!)\n",
        "df_grouped = df_final.groupby(\"unique_session_id\", as_index=False).agg({\n",
        "    \"date\": \"last\",\n",
        "    \"fullvisitor_id\": \"first\",\n",
        "    \"view_product_detail\": \"sum\",\n",
        "    \"add_to_cart\": \"sum\",\n",
        "    \"checkout\": \"sum\",\n",
        "    \"purchase\": \"sum\"\n",
        "})\n",
        "\n",
        "df_grouped['country']= f\"ALDO_{COUNTRY}\"\n",
        "\n",
        "# ✅ Convert boolean columns to integer (0 or 1)\n",
        "boolean_columns = [\"view_product_detail\", \"add_to_cart\", \"checkout\", \"purchase\"]\n",
        "df_grouped[boolean_columns] = df_grouped[boolean_columns].astype(int)\n",
        "\n",
        "# ✅ Save processed data locally\n",
        "df_grouped.to_csv(LOCAL_TEMP_FILE, index=False)\n",
        "\n",
        "# Upload Final Data to GCS\n",
        "print(\"\\n **Uploading Processed Data to GCS...**\")\n",
        "upload_result = subprocess.run([\"gsutil\", \"cp\", LOCAL_TEMP_FILE, GCS_OUTPUT_FILE], capture_output=True, text=True)\n",
        "\n",
        "if upload_result.returncode == 0:\n",
        "    print(\"**Data Successfully Uploaded to GCS!**\")\n",
        "    os.remove(LOCAL_TEMP_FILE)  # Remove temp file after upload\n",
        "else:\n",
        "    print(\"**Upload Failed! Debugging Output Below:**\")\n",
        "    print(upload_result.stderr)\n",
        "\n",
        "print(\"\\n Data saved at:\", GCS_OUTPUT_FILE)\n"
      ],
      "metadata": {
        "id": "SZ5xoQC6vchW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extract Page Path Information"
      ],
      "metadata": {
        "id": "6mf_DjO9wCsA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import pyarrow.parquet as pq\n",
        "import time\n",
        "import re\n",
        "import gcsfs\n",
        "import subprocess\n",
        "import html\n",
        "import os\n",
        "import numpy as np\n",
        "import gc\n",
        "\n",
        "\n",
        "# ✅ Define Paths\n",
        "BUCKET_NAME = \"mma-capstone\"\n",
        "COUNTRY = \"CA\"\n",
        "YEAR = 2023\n",
        "INPUT_FILE = f\"gs://{BUCKET_NAME}/GA/ga_sample1/banner=ALDO_{COUNTRY}/year={YEAR}/final/year_{YEAR}.parquet\"\n",
        "TEMP_FILE = \"/tmp/temp_page_paths.csv\"\n",
        "FINAL_TEMP_FILE = \"/tmp/temp_page_paths_final.csv\"\n",
        "GCS_OUTPUT_FILE = f\"gs://{BUCKET_NAME}/data_preprocessed/page_paths_{COUNTRY}_{YEAR}.parquet\"\n",
        "\n",
        "# ✅ Compile regex patterns\n",
        "SOURCE_PATTERN = re.compile(r'(?:[\\?&])([^&]*(?:_source|_src|source))=([^&]+)', re.IGNORECASE)\n",
        "EMAIL_TRAFFIC_PATTERN = re.compile(r'((?:[\\?&]|amp;)utm_medium=email(?:[\\?&]|$)|(?:[\\?&]|amp;)sc_src=email\\S*)', re.IGNORECASE)\n",
        "UTM_CATEGORY_PATTERN = re.compile(r'(?:[\\?&]|amp;)utm_category=([^&]+)', re.IGNORECASE)\n",
        "PRODUCT_PATTERN = re.compile(r'/p/(\\d+)', re.IGNORECASE)\n",
        "CATEGORY_PATTERN = re.compile(r'/(?:ca|us)/(?:en|fr)(?:[_-][A-Z]{2})?/([^?]+?)(?=/p/|\\?|$)', re.IGNORECASE)\n",
        "SEGMENT_PATTERN = re.compile(r'[\\?&]utm_segment\\s*=\\s*([^&]+)', re.IGNORECASE)\n",
        "GENDER_PATTERN = re.compile(r'[\\?&]utm_gender\\s*=\\s*([^&]+)', re.IGNORECASE)\n",
        "GAD_SOURCE_PATTERN = re.compile(r'[\\?&]gad_source=', re.IGNORECASE)\n",
        "\n",
        "# ✅ Function to extract relevant information from `page_path`\n",
        "def extract_info(url):\n",
        "    if not isinstance(url, str):  # Handle missing values\n",
        "        return [None] * 9\n",
        "\n",
        "    try:\n",
        "        # ✅ Decode HTML entities\n",
        "        url = html.unescape(url)\n",
        "\n",
        "        # ✅ Extract source (utm_source, gad_source, sc_src, etc.)\n",
        "        source_match = SOURCE_PATTERN.search(url)\n",
        "        traffic_source = source_match.group(2) if source_match else None\n",
        "\n",
        "        # ✅ Detect Google Ads explicitly\n",
        "        if GAD_SOURCE_PATTERN.search(url):\n",
        "            traffic_source = \"Google Ad\"\n",
        "\n",
        "        # ✅ Detect email traffic\n",
        "        is_email_traffic = bool(EMAIL_TRAFFIC_PATTERN.search(url))\n",
        "\n",
        "        # ✅ Extract utm_category\n",
        "        utm_category_match = UTM_CATEGORY_PATTERN.search(url)\n",
        "        utm_category = str(utm_category_match.group(1)) if utm_category_match else None\n",
        "\n",
        "        # ✅ Extract product ID\n",
        "        product_match = PRODUCT_PATTERN.search(url)\n",
        "        product_id = str(product_match.group(1)) if product_match else None\n",
        "\n",
        "        # ✅ Extract category path and split into **up to 3 category levels**\n",
        "        category_match = CATEGORY_PATTERN.search(url)\n",
        "        category_path = category_match.group(1) if category_match else None\n",
        "\n",
        "        if category_path:\n",
        "            category_parts = category_path.split(\"/\")[:3]  # Extract max 3 category levels\n",
        "            category_lv1, category_lv2, category_lv3 = (category_parts + [None] * 3)[:3]\n",
        "        else:\n",
        "            category_lv1, category_lv2, category_lv3 = None, None, None\n",
        "\n",
        "        # ✅ Extract utm_segment\n",
        "        utm_segment_match = SEGMENT_PATTERN.search(url)\n",
        "        utm_segment = str(utm_segment_match.group(1)) if utm_segment_match else None\n",
        "\n",
        "        # ✅ Extract utm_gender\n",
        "        gender_match = GENDER_PATTERN.search(url)\n",
        "        utm_gender = str(gender_match.group(1)) if gender_match else None\n",
        "\n",
        "        return [\n",
        "            traffic_source, is_email_traffic, utm_category,\n",
        "            product_id, category_lv1, category_lv2, category_lv3, utm_segment, utm_gender\n",
        "        ]\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error processing URL: {url} → {e}\")\n",
        "        return [None] * 9\n",
        "\n",
        "# ✅ Read the Parquet file in chunks\n",
        "chunksize = 500000\n",
        "columns_to_load = [\"fullvisitor_id\", \"page_path\", \"unique_session_id\", \"date\"]  # Load only relevant columns\n",
        "\n",
        "fs = gcsfs.GCSFileSystem()\n",
        "parquet_file = pq.ParquetFile(fs.open(INPUT_FILE, \"rb\"))\n",
        "\n",
        "chunk_number = 0\n",
        "df_list = []\n",
        "\n",
        "for batch in parquet_file.iter_batches(batch_size=chunksize, columns=columns_to_load):\n",
        "    chunk_number += 1\n",
        "    start_time = time.time()\n",
        "\n",
        "    chunk = batch.to_pandas()\n",
        "    chunk[\"page_path\"] = chunk[\"page_path\"].astype(str)  # Convert page_path to string\n",
        "    print(f\"\\n✅ Processing Chunk {chunk_number} - Rows: {chunk.shape[0]}\")\n",
        "\n",
        "    # ✅ Apply the extraction function\n",
        "    extracted_values = chunk[\"page_path\"].map(extract_info).apply(pd.Series)\n",
        "\n",
        "    # ✅ Rename extracted columns\n",
        "    extracted_values.columns = [\n",
        "        \"traffic_source\", \"is_email_traffic\", \"utm_category\",\n",
        "        \"product_id\", \"category_lv1\", \"category_lv2\", \"category_lv3\", \"utm_segment\", \"utm_gender\"\n",
        "    ]\n",
        "\n",
        "    # ✅ Merge extracted values with original chunk\n",
        "    chunk = pd.concat([chunk[[\"fullvisitor_id\", \"page_path\", \"unique_session_id\", \"date\"]], extracted_values], axis=1)\n",
        "    chunk = chunk.drop_duplicates()\n",
        "\n",
        "    df_list.append(chunk)\n",
        "    #write_header = not os.path.exists(TEMP_FILE) or chunk_number == 1  # ✅ Ensure header in first chunk\n",
        "    #chunk.to_csv(TEMP_FILE, mode=\"a\", index=False, header=write_header)\n",
        "\n",
        "    # ✅ Force garbage collection to free memory\n",
        "    del chunk\n",
        "    gc.collect()\n",
        "\n",
        "\n",
        "    # ✅ Print processing time\n",
        "    end_time = time.time()\n",
        "    print(f\"✅ Chunk {chunk_number} Processed in {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "# ✅ Combine & Save Final Data\n",
        "\n",
        "df_final = pd.concat(df_list, ignore_index=True)\n",
        "df_final = df_final.drop_duplicates()\n",
        "\n",
        "\n",
        "df_final.to_parquet(FINAL_TEMP_FILE, index=False)\n",
        "\n",
        "\n",
        "# ✅ Upload to GCS\n",
        "print(\"\\n**Uploading Final Deduplicated File to GCS...**\")\n",
        "upload_result = subprocess.run([\"gsutil\", \"cp\", FINAL_TEMP_FILE, GCS_OUTPUT_FILE], capture_output=True, text=True)\n",
        "\n",
        "if upload_result.returncode == 0:\n",
        "    print(\"✅ **Final Data Successfully Uploaded to GCS!**\")\n",
        "    os.remove(FINAL_TEMP_FILE)\n",
        "else:\n",
        "    print(\"❌ **Upload Failed! Debugging Output Below:**\")\n",
        "    print(upload_result.stderr)\n",
        "\n",
        "print(\"\\n **Processing Completed Successfully!** Data saved at:\", GCS_OUTPUT_FILE)\n"
      ],
      "metadata": {
        "id": "3x9wtxquwFaF"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}